"""
Module that defines a lexer using `ply.lex`.
It won't make your experiment harder if you don't read it.
"""

from functools import wraps
from typing import List

import ply.lex as lex

from frontend.ast import tree
from utils.error import DecafLexError

from .lex import *

error_stack: List[DecafLexError] = []

states = (("multiline", "exclusive"),)


@lex.TOKEN(r"/\*")
def t_multiline(t):
    t.lexer.begin("multiline")


@lex.TOKEN(r"\*/")
def t_multiline_end(t):
    t.lexer.begin("INITIAL")


t_multiline_ignore_all = rf".+?(?=\*/|{ t_ignore_Newline })"


@lex.TOKEN(t_ignore_Newline)
def t_ANY_Newline(t):
    t.lexer.lineno += 1


def t_ANY_error(t):
    error_stack.append(DecafLexError(t))
    t.lexer.skip(1)


# 将标识符转化为 AST 节点
def _identifier_into_node(f):
    @wraps(f)
    def wrapped(t):
        t = f(t)
        if t.type == "Identifier":
            t.value = tree.Identifier(t.value)
        return t

    return wrapped


t_Identifier = _identifier_into_node(t_Identifier)


# 将整数字面值转化为 AST 节点
def _intlit_into_node(f):
    @wraps(f)
    def wrapped(t):
        t = f(t)
        t.value = tree.IntLiteral(t.value)
        return t

    return wrapped


t_Integer = _intlit_into_node(t_Integer)

lexer = lex.lex()
lexer.error_stack = error_stack  # type: ignore
